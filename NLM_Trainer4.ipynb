{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jb-diplom/phd/blob/main/NLM_Trainer4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccHJqOajNN7l"
      },
      "source": [
        "#The Effect of Humour in Political Messaging: \n",
        "An investigation combining fine-tuned neural language models and social network analysis<br>\n",
        "by<br>\n",
        "Janice Butler: University of Amsterdam, Master Thesis 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKpiNY516JUf"
      },
      "source": [
        "## Introduction\n",
        "This notebook implements the fine-tuning of various neural language models (NLMs) based on a new corpus of annotated humorous texts.\n",
        "Two classifications are made\n",
        "\n",
        "1.   Degree of humour\n",
        "2.   Comic styles\n",
        "\n",
        "## Method \n",
        "The training script is modified from [run_glue.py](https://huggingface.co/transformers/examples.html#glue). \n",
        "The training is automatically tracked in the Weights & Biases dashboard. \n",
        "\n",
        "### Supervised Fine-Tuning\n",
        "\n",
        "This script fine-tunes NMLs on corpora scraped from several sub-reddits:\n",
        "* https://www.reddit.com/r/Jokes/\n",
        "* https://www.reddit.com/r/satire/\n",
        "* https://www.reddit.com/r/Showerthoughts/\n",
        "* https://www.reddit.com/r/SurrealHumor\n",
        "\n",
        "and from twitter:\n",
        "* https://twitter.com/midnight\n",
        "\n",
        "For non-humorous texts an equal amount of data was taken from these serious news outlets:\n",
        "* https://twitter.com/AP\n",
        "* https://twitter.com/BBCworld\n",
        "* https://twitter.com/ITN\n",
        "* https://twitter.com/ITVnews\n",
        "* https://twitter.com/SkyNewsPolitics\n",
        "* https://twitter.com/TheEconomist\n",
        "\n",
        "### Annotation\n",
        "The reddit data is automatically annotated into 5 grades  according to the up-votes per subreddit. All other annotation was achieved through manual categorisation, for humour degree on the https://twitter.com/midnight tweets and in all cases for type of humour. The categories being:\n",
        "* Serious\n",
        "* Fun\n",
        "* Benevolent humour\n",
        "* Wit\n",
        "* Nonsense\n",
        "* Irony\n",
        "* Satire\n",
        "* Sarcasm\n",
        "* Cynicism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt71k9Ew6Qp6"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "Pre-Trained Neural Language Models (NLMs) are taken from [the repository at Huggingface](https://huggingface.co/models). The generic Huggingface [Transformers API](https://huggingface.co/transformers/)  is used throughout for fine-tuning and the [Huggingface Pipeline API](https://huggingface.co/transformers/main_classes/pipelines.html) is taken for easy utilisation of the finished models\n",
        "\n",
        "### NLM Training-Performance Monitoring\n",
        "During fine-tuning a multitude of parameters are relayed to a data-base at https://wandb.ai/site. Additionally the fine-tuned model and all resultant meta-data for later cataloging of results and use with the model are recorded in projects defined [here](https://wandb.ai/jb-diplom) \n",
        " \n",
        "\n",
        "```\n",
        "TODO: add screenshot loaded from GIT\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Install the Hugging Face transformers and Weights & Biases libraries, and the dataset and training script for humour fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J05Pf8OnEyT"
      },
      "source": [
        "## Installation and Import of Required Packages and Libraries\n",
        "\n",
        "The dependencies are as follows:\n",
        "\n",
        "\n",
        "* Huggingface framework for loading and training models, preprocessing of data\n",
        "* Optionally install transformers datasets, but not needed if own data/project data is being used\n",
        "* Wandb is used for visualization of results on the project dashboard https://wandb.ai/jb-diplom/janice-demo\n",
        "* sentencepiece is required for deberta models\n",
        "* General purpose libraries (os, glob, pandas, numpy)\n",
        "* GUI and visualization libraries (data_table, ipywidgets, plotly, tqdm, matplotlib\n",
        "* For calculating accuracy of fine-tuned models and visualizing the results , sklearn.metrics is used\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUP7zxN2NKUi",
        "cellView": "form"
      },
      "source": [
        "#@markdown Do imports\n",
        "!pip install transformers -qq           # huggingface framework for loading and training models, preprocessing of data\n",
        "# Uncomment following line to carry out benchmark tests with hf datasets\n",
        "!pip install transformers datasets -qq  # currently transformers datasets --> add own data\n",
        "!pip install wandb -qq                  # for visualization of results on the project dashboard https://wandb.ai/jb-diplom/janice-demo\n",
        "!pip install sentencepiece              # required for deberta\n",
        "!pip install chart_studio\n",
        "#!pip install evaluate                   # needed for new run_glue scrtipt\n",
        "# this was the basis for the inital imlementation\n",
        "# !wget https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py -qq\n",
        "\n",
        "# Weights and Biases logging of training metrics and archiving of training results\n",
        "import wandb\n",
        "\n",
        "# General purpose libraries\n",
        "from   google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime \n",
        "\n",
        "# Visualization libraries\n",
        "%load_ext google.colab.data_table\n",
        "from   google.colab import data_table\n",
        "import ipywidgets as widgets\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from   tqdm.notebook import trange, tqdm\n",
        "import matplotlib.pyplot as plt # For multi plots\n",
        "\n",
        "# Hugging face API for loading pre-trained models, fine-tuning and utilization\n",
        "import transformers\n",
        "from   transformers import AutoModelForSequenceClassification, AutoConfig, pipeline\n",
        "\n",
        "# Stuff for calculating accuracy of fine-tuned models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn import metrics\n",
        "\n",
        "# Stuff for displaying metrics of fine-tuned models\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5DsG_bALsh"
      },
      "source": [
        "## API Key\n",
        "The following calls registers this run at Weights and Biases github unless a session is already active.\n",
        "Optionally, we can set environment variables to customize W&B logging. See [documentation](https://docs.wandb.com/library/integrations/huggingface).\n",
        "\n",
        "### Google Drive\n",
        "The project data is hosted on GDrive to enable an easy interface with [Google Colab](https://colab.research.google.com/). The training data and results are taken from and stored to directories on GDrive, which has to be mounted and requires appropriate credentials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3FFmY4JqWdN"
      },
      "source": [
        "#@markdown Connect to wandb\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = 'JanicesPhD'\n",
        "wandb.login(relogin='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-yWRxcu8kCB",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount GDrive\n",
        "drive.mount('/content/gdrive',True)\n",
        "file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/data/*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "HffRWEfM-Kg_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "uB74DMmeMqq9"
      },
      "source": [
        "#@markdown Set some global values for consistency of output styling\n",
        "plot_bgcolor='rgb(150,150,160)'\n",
        "cmap='viridis'\n",
        "color_palette_r = px.colors.sequential.Viridis_r\n",
        "color_palette   = px.colors.sequential.Viridis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb4QBSswX1HV"
      },
      "source": [
        "# Specify Parameters and Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNWjjJYhNQx3"
      },
      "source": [
        "Here you can choose which pre-trained NL model to fine-tune. Further options are:\n",
        "\n",
        "*   Which training-data to use\n",
        "*   Which project to save run-time data to\n",
        "*   The GLUE-Task to use\n",
        "*   Initial learning rate\n",
        "*   Number of epochs to train\n",
        "*   Stepsize for logging\n",
        "*   Whether to freeze layers\n",
        "*   Testrun with mini dataset or not\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYV7y-FvdAFT"
      },
      "source": [
        "#@title Enter Parameters for Training { vertical-output: true, form-width: \"50%\", display-mode: \"form\" }\n",
        "\n",
        "#@markdown Specify Parameters for Training\n",
        "#@markdown ---\n",
        "\n",
        "# Take viable names from https://huggingface.co/transformers/pretrained_models.html\n",
        "Comment = \"policy-noeu-roberta-large3e_mini\" #@param {type:\"string\"}\n",
        "Model = \"roberta-large\" #@param [\"bert-base-uncased\", \"distilbert-base-uncased\", \"gpt2\", \"distilgpt2\", \"gpt2-medium\", \"xlnet-base-cased\", \"roberta-base\", \"distilroberta-base\", \"t5-base\", \"microsoft/deberta-base\", \"google/electra-base-discriminator\", \"google/electra-large-discriminator\", \"vinai/bertweet-base\", \"nghuyong/ernie-3.0-base-zh\", \"nghuyong/ernie-2.0-large-en\", \"nghuyong/ernie-2.0-en\", \"distilgpt2\", \"gpt2-large\", \"roberta-large\"] {allow-input: true}\n",
        "GLUE_Task = \"\" #@param [\"\", \"cola\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\", \"GPT2\"]\n",
        "Initial_Learn_Rate = 2e-5 #@param {type: \"number\"}\n",
        "\n",
        "NrEpochs =   3#@param {type: \"number\"}\n",
        "Do_Train = True #@param {type:\"boolean\"}\n",
        "Do_Eval = True #@param {type:\"boolean\"}\n",
        "Do_Predict = True #@param {type:\"boolean\"}\n",
        " \n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Parameters for Quick Tests\n",
        "#@markdown ---\n",
        "do_quick_test = True #@param {type:\"boolean\"}\n",
        "Freeze_Layers = False #@param {type:\"boolean\"}\n",
        "max_train_samples = 30000 #@param {type:\"slider\", min:100, max:100000, step:100}\n",
        "max_val_samples = 3000 #@param {type:\"slider\", min:10, max:10000, step:10}\n",
        "max_test_samples = 3000 #@param {type:\"slider\", min:10, max:10000, step:10}\n",
        "#Percent_of_Trainingdata_to_use = 10 #@param {type:\"slider\", min: 5, max:100, step:5}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Visualization Parameters\n",
        "#@markdown ---\n",
        "\n",
        "Do_Visualization = True #@param {type:\"boolean\"}\n",
        "WandB_Project = \"janice-final\" #@param [\"thesis\", \"thesis-test-runs\", \"humour-type\", \"humour degree\", \"binary humour degree\", \"janice-final\"] {allow-input: true}\n",
        "Logging_Steps = 20 #@param {type:\"slider\", min:10, max:100, step:10}\n",
        "\n",
        "#@markdown Choose Files for Training \n",
        "#@markdown ---\n",
        "file_ext = \".tsv\" #@param [\".tsv\", \".csv\", \".json\"] {allow-input: true}\n",
        "own_modelid=Comment + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S').replace(' ','_').replace(':','.')\n",
        "print (\"New Model ID:\", own_modelid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Xmrt6_KWpwIM"
      },
      "source": [
        "#@markdown Which files would you like to use for traiing the NLM?\n",
        "\n",
        "dir_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/phd_data/*/\")\n",
        "dir_choice = widgets.Dropdown(options=dir_list,value=dir_list[0])\n",
        "# file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/data/*\"+ file_ext)\n",
        "# file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/msc_data/*\"+ file_ext)\n",
        "# file_list = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/midnight/*\"+ file_ext)\n",
        "\n",
        "file_list.insert(0,\"\")\n",
        "train_file = widgets.Dropdown(options=file_list,value=\"\")\n",
        "validation_file = widgets.Dropdown(options=file_list,value=\"\")\n",
        "test_file = widgets.Dropdown(options=file_list,value=\"\")\n",
        "\n",
        "items = [widgets.Label(value=\"Source Directory\"),\n",
        "         widgets.Label(value= \"Training\"),\n",
        "         widgets.Label(value=\"Validation\"),\n",
        "         widgets.Label(value=\"Test\")]\n",
        "\n",
        "left_box = widgets.VBox([items[0], items[1], items[2], items[3]],width='10%')\n",
        "right_box = widgets.VBox([dir_choice,train_file,validation_file,test_file],width='80%')\n",
        "file_pickers=widgets.HBox([left_box, right_box], width='100%')\n",
        "# file_pickers.overflow_x = 'auto'\n",
        "right_box.overflow_x = 'auto'\n",
        "\n",
        "def updateDoclist(b):\n",
        "    train_file.options=glob.glob(dir_choice.value + \"*train*\" + file_ext)\n",
        "    validation_file.options=glob.glob(dir_choice.value + \"*dev*\" + file_ext)\n",
        "    test_file.options=glob.glob(dir_choice.value + \"*test*\" + file_ext)\n",
        "\n",
        "dir_choice.observe(updateDoclist, names='value')\n",
        "display(file_pickers)\n",
        "updateDoclist(None)\n",
        "do_restart = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GO21yhdbj0Fi"
      },
      "source": [
        "#@markdown Optionally specify the restart of an aborted run\n",
        "#@markdown ---\n",
        "\n",
        "do_restart = False #@param {type:\"boolean\"}\n",
        "specify_model_path = \"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Checkpoint Name** (e.g. `checkpoint-12000`)\n",
        "specify_checkpoint = \"checkpoint-20000\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Model Name**<br>\n",
        "#@markdown Fetch model Name from the W & B workspace e.g. [here](https://wandb.ai/jb-diplom/janice-final/table?workspace=user-jb-diplom) <br>\n",
        "#@markdown Should be of the form <Comment> + <Timestamp> (e.g. `electra-L-htype_balanced20e2021-05-16_07.13.44`)\n",
        "original_modelname = 'bert-base-htype_balanced20e2021-05-16_07.15.13' #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pBSRh8kNRBi"
      },
      "source": [
        "os.environ['TRAINING_FILE']=train_file.value\n",
        "os.environ['VALID_FILE']=validation_file.value\n",
        "os.environ['TEST_FILE']=test_file.value\n",
        "os.environ['WANDB_PROJECT']=WandB_Project \n",
        "os.environ['WANDB_WANDB_TAGS']=train_file.value # can add comma separated additions, here\n",
        "os.environ['WANDB_JOB_TYPE']=(\"Testrun\" if do_quick_test else \"Fullrun\")\n",
        "os.environ['WANDB_LOG_MODEL'] = 'true'  # saving the model to wandb\n",
        "os.environ['WANDB_RUN_ID'] = own_modelid  # set own id to allow reuse in next cell\n",
        "os.environ['WANDB_WATCH']=\"all\"\n",
        "os.environ['WANDB_RESUME']=\"auto\"\n",
        "\n",
        "os.environ['GLUE_TASK_NAME']=GLUE_Task\n",
        "os.environ['TRAIN_EPOCHS']=str(NrEpochs)\n",
        "os.environ['MODEL']=Model\n",
        "os.environ['LR']=str(Initial_Learn_Rate)\n",
        "os.environ['LS']=str(Logging_Steps)\n",
        "os.environ['RUNNAME']=Comment\n",
        "os.environ['REPORT_TO']=\"wandb\"\n",
        "os.environ['OUTPUT_DIR']=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\"+Model\n",
        "os.environ['SAVE_STEPS']=\"50000\" # big step to avoid filling disk quota\n",
        "os.environ['SAVE_LIMIT']=\"1\"    # only one backup (let's live dangerously but save space)\n",
        "os.environ['BATCH_SIZE']=\"64\"    \n",
        "os.environ['SEQ_LENGTH']=\"256\"    \n",
        "\n",
        "if do_restart:\n",
        "  # to restart from checkpoint use following type of model path\n",
        "  # os.environ['MODEL']=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\"+Model+\"/checkpoint-12000/\"\n",
        "  # https://wandb.ai/jb-diplom/janice-final/runs/electra-L-htype_balanced20e2021-05-16_07.13.44\n",
        "  own_modelid=original_modelname\n",
        "  run_id=\"jb-diplom/\"+ WandB_Project + \"/\" + own_modelid\n",
        "  run=wandb.init(project=WandB_Project, entity='jb-diplom', id=wandb.Api().run(run_id).id, resume='allow')\n",
        "  os.environ['MODEL']=\"/content/gdrive/MyDrive/ColabNotebooks/SavedModels/\"+Model+\"/\" + specify_checkpoint + \"/\"\n",
        "\n",
        "if (Do_Visualization):\n",
        "  os.environ['REPORT_TO']=\"wandb\"\n",
        "\n",
        "# %env\n",
        "#  --task_name $GLUE_TASK_NAME \\\n",
        "#  --jb_task_name \"t5\" \\\n",
        "# --adafactor --lr_scheduler_type cosine --warmup_ratio 0.1 \\\n",
        "\n",
        "if do_quick_test:\n",
        "  os.environ['TRAIN_SAMPLES']=(str(max_train_samples) if do_quick_test else \"\")\n",
        "  os.environ['VAL_SAMPLES']=  (str(max_val_samples)   if do_quick_test else \"\")\n",
        "  os.environ['TEST_SAMPLES']= (str(max_test_samples)  if do_quick_test else \"\")\n",
        "  \n",
        "  !python '/content/gdrive/MyDrive/ColabNotebooks/Visualization/run_glue3.py' \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --max_val_samples $VAL_SAMPLES \\\n",
        "    --max_test_samples $TEST_SAMPLES \\\n",
        "    --max_train_samples $TRAIN_SAMPLES \\\n",
        "    --tokenizer_name $MODEL \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --max_seq_length $SEQ_LENGTH \\\n",
        "    --per_device_train_batch_size $BATCH_SIZE \\\n",
        "    --per_device_eval_batch_size=$BATCH_SIZE \\\n",
        "    --learning_rate $LR \\\n",
        "    --num_train_epochs $TRAIN_EPOCHS \\\n",
        "    --output_dir $OUTPUT_DIR \\\n",
        "    --overwrite_output_dir \\\n",
        "    --logging_steps $LS \\\n",
        "    --pad_to_max_length \\\n",
        "    --run_name $RUNNAME \\\n",
        "    --report_to $REPORT_TO \\\n",
        "    --train_file $TRAINING_FILE \\\n",
        "    --validation_file $VALID_FILE \\\n",
        "    --test_file $TEST_FILE \\\n",
        "    --save_steps $SAVE_STEPS \\\n",
        "    --save_total_limit $SAVE_LIMIT \\\n",
        "    --fp16 \\\n",
        "    --optim adafactor --lr_scheduler_type cosine \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --skip_memory_metrics\n",
        "else:\n",
        "  !python '/content/gdrive/MyDrive/ColabNotebooks/Visualization/run_glue3.py' \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --tokenizer_name $MODEL \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --max_seq_length $SEQ_LENGTH \\\n",
        "    --per_device_train_batch_size $BATCH_SIZE \\\n",
        "    --per_device_eval_batch_size=$BATCH_SIZE \\\n",
        "    --learning_rate $LR \\\n",
        "    --num_train_epochs $TRAIN_EPOCHS \\\n",
        "    --output_dir $OUTPUT_DIR \\\n",
        "    --overwrite_output_dir \\\n",
        "    --logging_steps $LS \\\n",
        "    --run_name $RUNNAME \\\n",
        "    --report_to $REPORT_TO \\\n",
        "    --train_file $TRAINING_FILE \\\n",
        "    --validation_file $VALID_FILE \\\n",
        "    --test_file $TEST_FILE \\\n",
        "    --save_steps $SAVE_STEPS \\\n",
        "    --save_total_limit $SAVE_LIMIT \\\n",
        "    --fp16 \\\n",
        "    --optim adafactor --lr_scheduler_type cosine \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --skip_memory_metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FrJdV7eMAbG"
      },
      "source": [
        "# Testing of fine-tuned models\n",
        "Retrieve model from W&B repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKRacEzX6ifg"
      },
      "source": [
        "# run = wandb.init()\n",
        "run= wandb.init(project=WandB_Project, entity='jb-diplom')\n",
        "# Take this from just finished run or from one of your \n",
        "# favorite fine-tuned models at https://wandb.ai/jb-diplom/janice-full/artifacts\n",
        "\n",
        "#@markdown Enter Model Type and Name to be retreived from https://wandb.ai\n",
        "Data_Type = \"Policy (no EU)\" #@param [\"Humour Type\", \"Humour Degree\", \"H-Degree (binary)\", \"Policy Type\", \"Emotion2\", \"EU (binary)\", \"Policy (no EU)\"]\n",
        "use_latest_model = False #@param {type:\"boolean\"}\n",
        "if use_latest_model:\n",
        "  model_id=own_modelid\n",
        "else:\n",
        "  model_id = 'policy-noeu-roberta-large5e_all2023-01-05_09.41.04'  #@param {type: \"string\"}\n",
        "  own_modelid = model_id\n",
        "\n",
        "# Set human-readable labels\n",
        "leng = 1\n",
        "policy_labels=[]\n",
        "leng = 1  # 1 for humour stuff, 2 for policy\n",
        "\n",
        "if Data_Type == \"Policy Type\" :\n",
        "  leng = 2  # 1 for humour stuff, 2 for policy\n",
        "  policy_labels=['10','11','20','30','40','41','50','60','70']\n",
        "  label_map= {\n",
        "      \"LABEL_0\": 'Ext. Rel.',\n",
        "      \"LABEL_1\": 'EU',\n",
        "      \"LABEL_2\": 'Democracy',\n",
        "      \"LABEL_3\": 'Political System',\n",
        "      \"LABEL_4\": 'Economy',\n",
        "      \"LABEL_5\": 'Growth',\n",
        "      \"LABEL_6\": 'Welfare',\n",
        "      \"LABEL_7\": 'Society',\n",
        "      \"LABEL_8\": 'Social Grps',\n",
        "      }\n",
        "elif Data_Type == \"Policy (no EU)\" :\n",
        "  leng = 2  # 1 for humour stuff, 2 for policy\n",
        "  policy_labels=['10','20','30','40','41','50','60','70']\n",
        "  label_map= {\n",
        "      \"LABEL_0\": 'Ext. Rel.',\n",
        "      \"LABEL_1\": 'Democracy',\n",
        "      \"LABEL_2\": 'Political System',\n",
        "      \"LABEL_3\": 'Economy',\n",
        "      \"LABEL_4\": 'Growth',\n",
        "      \"LABEL_5\": 'Welfare',\n",
        "      \"LABEL_6\": 'Society',\n",
        "      \"LABEL_7\": 'Social Grps',\n",
        "      }\n",
        "elif Data_Type == \"Emotion2\" :\n",
        "  policy_labels=['0','1','2','3','4','5','6']\n",
        "  label_map= {\n",
        "      \"LABEL_0\": \"anger\",\n",
        "      \"LABEL_1\": \"disgust\",\n",
        "      \"LABEL_2\": \"fear\",\n",
        "      \"LABEL_3\": \"joy\",\n",
        "      \"LABEL_4\": \"neutral\",\n",
        "      \"LABEL_5\": \"sadness\",\n",
        "      \"LABEL_6\": \"surprise\"\n",
        "      }\n",
        "elif Data_Type == \"Humour Type\" :\n",
        "  policy_labels=['0','1','2','3','4','5','6','7','8']\n",
        "  label_map= {\n",
        "      \"LABEL_0\": 'serious',\n",
        "      \"LABEL_1\": 'fun',\n",
        "      \"LABEL_2\": 'benevolent',\n",
        "      \"LABEL_3\": 'wit',\n",
        "      \"LABEL_4\": 'nonsense',\n",
        "      \"LABEL_5\": 'irony',\n",
        "      \"LABEL_6\": 'satire',\n",
        "      \"LABEL_7\": 'sarcasm',\n",
        "      \"LABEL_8\": 'cynicism'\n",
        "      }\n",
        "elif Data_Type == \"EU (binary)\" :\n",
        "  policy_labels=['0','1']\n",
        "  label_map= {\n",
        "      \"LABEL_1\": 'EU',\n",
        "      \"LABEL_0\": 'non-EU'\n",
        "      }\n",
        "else :\n",
        "  policy_labels=['0','1','2','3','4','5','6','7','8']\n",
        "\n",
        "names=['serious','fun','benevolent','wit','nonsense','irony','satire','sarcasm','cynicism']\n",
        "degree_names=['serious','wry smile','smile','grin','very funny','hilarious']\n",
        "policy_names=['Ext. Rel.','EU','Democracy','Political System','Economy','Growth','Welfare','Society','Social Grps']\n",
        "emotion_names=['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
        "binary_degree_names=['serious','funny']\n",
        "name_lst=list(label_map.values())  # generalized\n",
        "\n",
        "model_root= 'jb-diplom/' + WandB_Project + '/model-'\n",
        "model_path= model_root + model_id + ':v0'\n",
        "print(\"Retreiving artefact:\", model_path)\n",
        "artifact = run.use_artifact(model_path, type='model')\n",
        "# artifact = run.use_artifact('jb-diplom/janice-full/model-219xio3e:v0', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "print(\"Model saved locally to:\", artifact_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2jX4Te7yUMn"
      },
      "source": [
        "# Load fine-tuned model\n",
        "from transformers import GPT2TokenizerFast,AutoTokenizer,GPT2ForSequenceClassification\n",
        "from transformers import AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "\n",
        "model_path='/content/artifacts/model-' + model_id + ':v0' # The model just downloaded from wandb.io\n",
        "\n",
        "humour_classif = ''\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "humour_classif = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    framework=\"pt\",\n",
        "    device=0,\n",
        "    task='sentiment-analysis',\n",
        "    # top_k=None\n",
        "    # return_all_scores = True,\n",
        ")\n",
        "\n",
        "multi_humour_classif = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    framework=\"pt\",\n",
        "    device=0,\n",
        "    task='sentiment-analysis',\n",
        "#    return_all_scores = True,\n",
        "    # top_k=None\n",
        ")\n",
        "  # humour_classif = pipeline('sentiment-analysis',model_path)\n",
        "\n",
        "# This text classification pipeline can currently be loaded from pipeline() using the following task \n",
        "# identifier: \"sentiment-analysis\"\n",
        "#@markdown ***Create Pipeline***<br>\n",
        "#@markdown Enter a test string to check whether the downloaded model is working\n",
        "test_string = 'i love the european commission'  #@param {type: \"string\"}\n",
        "\n",
        "multi_humour_classif(test_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWfk3Nt4jDgD"
      },
      "source": [
        "#@markdown Load file (specified for testing above) for manual testing\n",
        "# Take test.tsv and compare expected with actual results\n",
        "test_df = pd.read_csv(test_file.value, delimiter='\\t', header=None, \n",
        "                        lineterminator='\\n',encoding='utf-8')\n",
        "cols=['Text','Humour Level']\n",
        "test_df.columns=cols\n",
        "# test_df.describe()\n",
        "data_table.DataTable(test_df, include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IKgPZHCkjOM",
        "cellView": "form"
      },
      "source": [
        "# Do a little test against a chosen test-dataset\n",
        "#@markdown #### **Attention:** This Operation may take some hours, depending on the quantity of test data and model inference speed!\n",
        "#@markdown ----\n",
        "#@markdown ###How many records would you like to test?\n",
        "#@markdown An entry of -1 implies processing of **ALL** records\n",
        "#@markdown Which type of model is being analysed?\n",
        "Data_Type = \"Policy (no EU)\" #@param [\"Humour Type\", \"Humour Degree\", \"H-Degree (binary)\", \"Policy Type\", \"Emotion2\", \"EU (binary)\", \"Policy (no EU)\"]\n",
        "sample_nr =   10000#@param {type: \"number\"}\n",
        "\n",
        "# tweet_df.iloc[1:5, 1:1]\n",
        "content=test_df.iloc[:,0]\n",
        "labels=test_df.iloc[:,1]  # NB: 2 digit numbers for collapsed policy types\n",
        "humour=[]\n",
        "y_true=[]\n",
        "y_score=[]\n",
        "\n",
        "# establish humour content of tweet from fine-tuned model\n",
        "hit=0\n",
        "miss=0\n",
        "out_by_one=0\n",
        "\n",
        "# names=['serious','fun','benevolent','wit','nonsense','irony','satire','sarcasm','cynicism']\n",
        "# degree_names=['serious','wry smile','smile','grin','very funny','hilarious']\n",
        "# policy_names=['Ext. Rel.','EU','Democracy','Political System','Economy','Growth','Welfare','Society','Social Grps']\n",
        "# emotion_names=['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
        "\n",
        "for i, tweet in tqdm(enumerate(content.head(sample_nr)),total=(len(content) if sample_nr==-1 else sample_nr)):\n",
        "  # clip to max_seq_length, extract the number from the label of the result \n",
        "  cls_val=humour_classif('{:1.512}'.format(tweet))\n",
        "  val=cls_val[0]['label'][6:7:1]\n",
        "  try:\n",
        "    if (int(policy_labels[int(val)]) == int(labels[i][0:leng:1])):\n",
        "      y_true.append(1)\n",
        "      hit +=1\n",
        "    else:\n",
        "      miss +=1\n",
        "      y_true.append(0)\n",
        "      if abs(int(policy_labels[int(val)]) - int(labels[i][0:leng:1])) <3:\n",
        "        out_by_one += 1\n",
        "    humour.append(policy_labels[int(val)])\n",
        "    y_score.append(cls_val[0]['score'])\n",
        "  except:\n",
        "    print (\".\")\n",
        "    continue\n",
        "\n",
        "print (\"Hits:\", hit,\"\\nMisses:\",miss,\"\\nOut by one:\", out_by_one, \"\\n%-age of hits:\", (hit*100)/(hit+miss))\n",
        "# '{:1.35}'.format('12345678901234567890') use to truncate string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qny8fgXhozO"
      },
      "source": [
        "#@title Analyse Results of Training\n",
        "\n",
        "#@markdown Which type of model is being analysed?\n",
        "#Data_Type = \"Policy Type\" #@param [\"Humour Type\", \"Humour Degree\", \"H-Degree (binary)\", \"Policy Type\"]\n",
        "\n",
        "#@markdown To assess the statistical success of the fine-tuning, 3 analyses are conducted using the test data-set:\n",
        "#@markdown  **TODO** add option for humour degree (0to5) or txype (0to8)\n",
        "\n",
        "#@markdown * **Confusion Matrix**<br>\n",
        "#@markdown  A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
        "#@markdown Here the classifiers are ether humour type or degree of humour. The correctly predicted results (as compared against the expected results in the test data) are\n",
        "#@markdown those on the diagonal. The numbers within the matrix represent the following four cases\n",
        "#@markdown * true positives (TP): These are cases in which the trained NLM predicted the correct value (of type or degree)\n",
        "#@markdown * true negatives (TN): These are cases in which the trained NLM predicted correctly that the value (of type or degree) is not fitting for the test text\n",
        "#@markdown * false positives (FP): These are cases in which the trained NLM predicted incorrectly that the value (of type or degree) would match for the test text\n",
        "#@markdown * false negatives (FN): These are cases in which the trained NLM predicted incorrectly that the value (of type or degree) is not fitting for the test text\n",
        "#@markdown To help with interpretation of the results, the first matrix is additionally normalized according to the proportion of all values for each indicator counted individually for the test run\n",
        "#@markdown <br>For more details consult \n",
        "#@markdown [scikit Confusion Matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
        "\n",
        "#@markdown * **ROC Curve**<br>\n",
        "#@markdown  This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class \n",
        "#@markdown <br>For more details consult \n",
        "#@markdown [scikit ROC](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)\n",
        "\n",
        "#@markdown * **Precision Recall**<br>\n",
        "#@markdown  Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. \n",
        "#@markdown In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n",
        "#@markdown The precision-recall curve shows the tradeoff between precision and recall for different threshold. \n",
        "#@markdown A high area under the curve represents both high recall and high precision, where high precision relates to a low false \n",
        "#@markdown positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results \n",
        "#@markdown (high precision), as well as returning a majority of all positive results (high recall).<br>\n",
        "#@markdown Average precision (**AP**) summarizes such a plot as the weighted mean of precisions achieved at each threshold, \n",
        "#@markdown with the increase in recall from the previous threshold used as the weight. <br>For more details consult \n",
        "#@markdown [scikit Precision Recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n",
        "\n",
        "clean_h=[]\n",
        "for val in humour:\n",
        "  clean_h.append(val.replace('\\r',\"\"))\n",
        "clean_p=[]\n",
        "pred=test_df['Humour Level'].tolist()[1:len(clean_h)+1:1]\n",
        "for val in pred:\n",
        "  clean_p.append(val.replace(' ',\"\").replace('\\r',\"\"))\n",
        "\n",
        "fig = plt.figure()\n",
        "cm_n =''\n",
        "cm = ''\n",
        "if Data_Type == \"Humour Type\":\n",
        "  cm_n=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5','6','7','8'], normalize= 'pred')\n",
        "  cm=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5','6','7','8'])\n",
        "  cm_n_display = ConfusionMatrixDisplay(cm_n, display_labels=names)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=names)\n",
        "elif Data_Type == \"Humour Degree\" :\n",
        "  cm_n=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5'], normalize= 'pred')\n",
        "  cm=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5'])\n",
        "  cm_n_display = ConfusionMatrixDisplay(cm_n, display_labels=degree_names)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=degree_names)\n",
        "elif Data_Type == \"Policy Type\" :\n",
        "  cm_n=confusion_matrix(clean_p,clean_h, labels=['10','11','20','30','40','41','50','60','70'], normalize= 'pred')\n",
        "  cm=confusion_matrix(clean_p,clean_h, labels=['10','11','20','30','40','41','50','60','70'])\n",
        "  cm_n_display = ConfusionMatrixDisplay(cm_n, display_labels=name_lst)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=name_lst)\n",
        "elif Data_Type == \"Policy (no EU)\" :\n",
        "  cm_n=confusion_matrix(clean_p,clean_h, labels=['10','20','30','40','41','50','60','70'], normalize= 'pred')\n",
        "  cm=confusion_matrix(clean_p,clean_h, labels=['10','20','30','40','41','50','60','70'])\n",
        "  cm_n_display = ConfusionMatrixDisplay(cm_n, display_labels=name_lst)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=name_lst)\n",
        "elif Data_Type == \"Emotion2\" :\n",
        "  cm_n=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5','6'], normalize= 'pred')\n",
        "  cm=confusion_matrix(clean_p,clean_h, labels=['0','1','2','3','4','5','6'])\n",
        "  cm_n_display = ConfusionMatrixDisplay(cm_n, display_labels=name_lst)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=name_lst)\n",
        "else :\n",
        "  cm_n=confusion_matrix(clean_p,clean_h, labels=['0','1'], normalize= 'pred')\n",
        "  cm=confusion_matrix(clean_p,clean_h, labels=['0','1'])\n",
        "  cm_n_display = ConfusionMatrixDisplay(cm_n, display_labels=binary_degree_names)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=binary_degree_names)\n",
        "\n",
        "fpr, tpr,  _ = roc_curve(y_true, y_score, pos_label=1)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "# fig2, ax2 = plt.subplots(figsize=(10,10))\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"\")\n",
        "\n",
        "avg_precision = average_precision_score(y_true, y_score)\n",
        "prec, recall, _ = precision_recall_curve(y_true, y_score, pos_label=1)\n",
        "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall,average_precision=avg_precision,estimator_name=\"\" )\n",
        "\n",
        "# Try a multi plot :-)\n",
        "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 20))\n",
        "fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
        "\n",
        "cm_n_display.plot(ax=axs[0,0],cmap='magma')\n",
        "axs[0,0].set_title('Normalized Confusion Matrix')\n",
        "cm_display.plot(ax=axs[0,1],cmap='magma')\n",
        "axs[0,1].set_title('Raw Confusion Matrix')\n",
        "roc_display.plot(ax=axs[1,0])\n",
        "axs[1,0].set_title('ROC Plot')\n",
        "pr_display.plot(ax=axs[1,1])\n",
        "axs[1,1].set_title('Precision Recall Plot')\n",
        "# plt.show()\n",
        "\n",
        "# create image to save to wandb\n",
        "plotname= \"conf_matrix-\" + own_modelid + \".png\"\n",
        "plot_dir = \"/content/gdrive/MyDrive/ColabNotebooks/Visualization/plots/\"\n",
        "fig.savefig(plot_dir + plotname)\n",
        "\n",
        "# TODO : extend this to multi-class precision recall. See example here\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOuh3KfveTT8"
      },
      "source": [
        "#@title Calculate Metrics for Precision, Recall and F1\n",
        "\n",
        "#@markdown Compute precision, recall, F-measure and support for each class <br>The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives\n",
        "#@markdown * The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "#@markdown * The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. <br>The recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "#@markdown * The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
        "#@markdown * The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
        "\n",
        "#@markdown **NB:** The metrics and image of the Confusion Matrix, ROC plot and Precision/Recall plot are all archived to WandB\n",
        "# print( \"F1 none:\",f1_score(clean_h,clean_p, average=None))\n",
        "from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\n",
        "print( \"Prec, Recall, F-Score (macro):\",precision_recall_fscore_support(clean_h, clean_p, average='macro'))\n",
        "print( \"Prec, Recall, F-Score (micro):\",precision_recall_fscore_support(clean_h, clean_p, average='micro'))\n",
        "prf_weighted = precision_recall_fscore_support(clean_h, clean_p, average='weighted')\n",
        "print( \"Prec, Recall, F-Score (weighted):\",prf_weighted)\n",
        "print( \"Prec, Recall, F-Score (per category):\",precision_recall_fscore_support(clean_h, clean_p, average=None))\n",
        "\n",
        "mcc = matthews_corrcoef(clean_h, clean_p)\n",
        "print(\"Matthews Coeff:\",mcc)\n",
        "#@markdown ---\n",
        "#@markdown If required, specify that plots and metrics be saved together with the model at [W & B](https://wandb.io)\n",
        "\n",
        "save_to_WandB = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_to_WandB:\n",
        "  # relay metrics to WandB\n",
        "  api = wandb.Api()\n",
        "  # run = api.run(\"jb-diplom/\"+ WandB_Project + \"/\" + own_modelid)\n",
        "\n",
        "  run_id=\"jb-diplom/\"+ WandB_Project + \"/runs/\" + own_modelid\n",
        "  run = api.run(run_id)\n",
        "\n",
        "  run.summary[\"precision\"] = prf_weighted[0]\n",
        "  run.summary[\"recall\"] = prf_weighted[1]\n",
        "  run.summary[\"MCC\"] = mcc\n",
        "  run.summary[\"F1\"] = prf_weighted[2]\n",
        "  run.summary.update()\n",
        "\n",
        "  # Send saved image of plots to WandB\n",
        "  # run_id=\"jb-diplom/\"+ WandB_Project + \"/\" + own_modelid\n",
        "  wandb.init(project=WandB_Project, entity='jb-diplom', id=wandb.Api().run(run_id).id, resume='allow')\n",
        "  im = plt.imread(plot_dir + plotname) # from previous cell\n",
        "  wandb.log({\"img\": [wandb.Image(im, caption=plotname)]})\n",
        "  wandb.finish()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in tweets, do humour evaluation and write results back to df and save new csv\n",
        "#@title Optionally Enter Alternative File Name for Conducting Test{ vertical-output: true, form-width: \"50%\", display-mode: \"form\" } (Prototype to replace the equivalent 2 cells down below)\n",
        "\n",
        "#@markdown #### Specify Source for inference (skip this if you wish to continue with the results obtained from the previous step). You should choose either:\n",
        "#@markdown - From the raw data directory, a set of tweets from one of various groupings\n",
        "#@markdown \n",
        "#@markdown or\n",
        "#@markdown - From the results directory, a set of tweets which has already had results fro a previous infernce run added\n",
        "#@markdown \n",
        "#@markdown **Tip**: reload the cell to repopulate the selection boxes\n",
        "\n",
        "Options = \"eu_influencer_tweets\" #@param [\"comedian-tweets250\", \"journalist-tweets\", \"ukmp_tweets\", \"eu_committee_tweets\", \"eu_influencer_tweets\"] {allow-input: true}\n",
        "basedir = glob.glob(\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/phd_data/*/\")\n",
        "dir_choice2 = widgets.Dropdown(options=dir_list,value=dir_list[0])\n",
        "file_list.insert(0,\"\")\n",
        "file_names = widgets.Dropdown(options=file_list,value=\"\")\n",
        "items = [widgets.Label(value=\"Directory\"),\n",
        "         widgets.Label(value= \"Source\")]\n",
        "def updateSourcelist(b):\n",
        "    file_names.options=glob.glob(dir_choice2.value + Options + \"*\" + file_ext)\n",
        "\n",
        "left_box = widgets.VBox([items[0], items[1]],width='10%')\n",
        "right_box = widgets.VBox([dir_choice2,file_names],width='80%')\n",
        "file_pickers2=widgets.HBox([left_box, right_box], width='100%')\n",
        "right_box.overflow_x = 'auto'\n",
        "\n",
        "dir_choice2.observe(updateSourcelist, names='value')\n",
        "display(file_pickers2)\n",
        "updateSourcelist(None)\n"
      ],
      "metadata": {
        "id": "5FRglGOxsq90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read and Display Sample of Loaded Data for Analysis\n",
        "\n",
        "tweet_df = pd.read_csv(file_names.value, delimiter='\\t',encoding='utf-16')\n",
        "# tweet_df.pop(tweet_df.columns[0])\n",
        "# tweet_df.pop(tweet_df.columns[3])\n",
        "data_table.DataTable(tweet_df, include_index=False, num_rows_per_page=10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6Q5I12lRzUmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2I_h3Cw6sj5",
        "cellView": "form"
      },
      "source": [
        "use_all_samples = False #@param {type:\"boolean\"}\n",
        "sample_nr = 1000 #@param {type:\"slider\", min:100, max:250000, step:100}\n",
        "output_max= sample_nr\n",
        "\n",
        "if use_all_samples:\n",
        "  sample_nr = -1\n",
        "  output_max = len(tweet_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate in Pipeline and Save (sample nr. or all) Results to GDrive\n",
        "#@markdown The results are added as a column corresponding to the current trait (e.g. Humour, Policy, Emotion, EU) to the test (input) data and are saved with a suffix (the model name) as `.tsv` file in the adjacent 'results' directory\n",
        "\n",
        "content=tweet_df.iloc[:,1]  # grab the content column (it should be the 2nd)\n",
        "trait=[]\n",
        "score=[]\n",
        "\n",
        "for tweet in tqdm(content.head(sample_nr)):\n",
        "  res = humour_classif(tweet)[0]\n",
        "  trait.append(res['label'])\n",
        "  score.append(res['score'])\n",
        "\n",
        "hlen=len(trait)\n",
        "for i in range (hlen, len(tweet_df)):\n",
        "  trait.append(\"not evaluated\")\n",
        "  score.append(0)\n",
        "\n",
        "import re\n",
        "Data_Type_name=re.sub(r'\\W+', '', Data_Type) # remove any non-alphanumerics\n",
        "\n",
        "# add trait column to dataframe\n",
        "tweet_df[Data_Type_name]=trait\n",
        "tweet_df[Data_Type_name+'_score']=score\n",
        "for feature in label_map.keys():\n",
        "  tweet_df[Data_Type_name] = tweet_df[Data_Type_name].replace(feature,label_map.get(feature),regex=True)\n",
        "\n",
        "# save results\n",
        "tweet_file_out=dir_choice2.value + '../results/' + Options +'_'+ Data_Type_name +'_' + model_id + \".tsv\"\n",
        "print (\"Saving to:\", tweet_file_out)\n",
        "\n",
        "# tweet_df.to_csv(tweet_file_out, sep='\\t', index=False, lineterminator='\\n',encoding='utf-16')\n",
        "tweet_df.to_csv(tweet_file_out, sep='\\t', index=False,encoding='utf-16')\n",
        "\n",
        "data_table.DataTable(tweet_df.head(min(sample_nr,20000)), include_index=False, num_rows_per_page=10)"
      ],
      "metadata": {
        "id": "kBvxgHXmCtDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn4Zr64i2ZI5"
      },
      "source": [
        "# Read in tweets, do humour evaluation and write results back to df and save new csv\n",
        "#@title (Legacy) Optionally Enter Alternative File Name for Conducting Test{ vertical-output: true, form-width: \"50%\", display-mode: \"form\" }\n",
        "\n",
        "#@markdown #### Specify Source for Humour Tests (skip this if you wish to continue with the results obtained from the previous step)\n",
        "#@markdown ---\n",
        "\n",
        "# basedir=\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/phd_data/raw_tweets/\"#@param {type:\"string\"}\n",
        "basedir = \"/content/gdrive/MyDrive/ColabNotebooks/Visualization/phd_data/raw_tweets/\" #@param [\"/content/gdrive/MyDrive/ColabNotebooks/Visualization/phd_data/raw_tweets/\"] {allow-input: true}\n",
        "file_name = \"eu_influencer_tweets\" #@param [\"comedian-tweets250\", \"journalist-tweets\", \"ukmp_tweets\", \"eu_committee_tweets\", \"eu_influencer_tweets\"] {allow-input: true}\n",
        "ext = \".tsv\" #@param [\".tsv\", \".csv\"]\n",
        "tweet_file=basedir + file_name + ext\n",
        "print (tweet_file)\n",
        "dtyps={'tweetId':str, 'content':str, 'username':str,'followers':int, 'conversationId':str, \n",
        "         'replyCount':int, 'retweetCount':int, 'likeCount':int, 'quoteCount':int}\n",
        "\n",
        "# tweet_df = pd.read_csv(tweet_file, delimiter='\\t', header=None, dtype=dtyps,lineterminator='\\n',encoding='utf-16')\n",
        "# tweet_df = pd.read_csv(tweet_file, delimiter='\\t', header=None, dtype=dtyps,encoding='utf-16')\n",
        "# cols=['tweetId', 'content', 'username','followers', 'conversationId', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount']\n",
        "\n",
        "tweet_df = pd.read_csv(tweet_file, delimiter='\\t',encoding='utf-16')\n",
        "# tweet_df.pop(tweet_df.columns[0])\n",
        "# tweet_df.pop(tweet_df.columns[3])\n",
        "\n",
        "tweet_df.head(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCqnN2_JPx2l",
        "cellView": "form"
      },
      "source": [
        "#@title (Legacy) Evaluate in Pipeline and Save (sample nr. or all) Results to GDrive\n",
        "#@markdown The results are added as a column corresponding to the current trait (e.g. Humour, Policy, Emotion, EU) to the test (input) data and are saved with a suffix (the model name) as `.tsv` file in the adjacent 'results' directory\n",
        "\n",
        "content=tweet_df.iloc[:,1]  # grab the content column (it should be the 2nd)\n",
        "trait=[]\n",
        "score=[]\n",
        "\n",
        "for tweet in tqdm(content.head(sample_nr)):\n",
        "  res = humour_classif(tweet)[0]\n",
        "  trait.append(res['label'])\n",
        "  score.append(res['score'])\n",
        "\n",
        "hlen=len(trait)\n",
        "for i in range (hlen, len(tweet_df)):\n",
        "  trait.append(\"not evaluated\")\n",
        "  score.append(0)\n",
        "\n",
        "# add trait column to dataframe\n",
        "tweet_df[Data_Type]=trait\n",
        "tweet_df[Data_Type+'_score']=score\n",
        "for feature in label_map.keys():\n",
        "  tweet_df[Data_Type] = tweet_df[Data_Type].replace(feature,label_map.get(feature),regex=True)\n",
        "\n",
        "# save results\n",
        "tweet_file_out=basedir + '../results/' + file_name +'_' + model_id + \".tsv\"\n",
        "print (\"Saving to:\", tweet_file_out)\n",
        "\n",
        "# tweet_df.to_csv(tweet_file_out, sep='\\t', index=False, lineterminator='\\n',encoding='utf-16')\n",
        "tweet_df.to_csv(tweet_file_out, sep='\\t', index=False,encoding='utf-16')\n",
        "\n",
        "data_table.DataTable(tweet_df.head(min(sample_nr,20000)), include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjo75aP7zVv6",
        "collapsed": true
      },
      "source": [
        "#@title Display distribution of data for each type of humour\n",
        "vc=tweet_df[Data_Type_name].value_counts()\n",
        "vc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-YiH3KZ7hgZ"
      },
      "source": [
        "#@title Display resulting data\n",
        "#@markdown Assuming very large numbers of processed tweets (> 20'000) you can specify a higher \n",
        "#@markdown threshold in terms of numbers of likes to filter out the less relevant data.<br><br>\n",
        "#@markdown Choose the minimum number of likes to reduce the displayed and visualized data\n",
        "tweet_df = tweet_df.dropna()\n",
        "min_nr_of_likes = 900 #@param {type:\"slider\", min:100, max:50000, step:100}\n",
        "\n",
        "nr_likes_str=\"{}\".format(min_nr_of_likes)\n",
        "print(nr_likes_str)\n",
        "query_str = \"'\" + Data_Type_name +\"'!='not evaluated' & likeCount > \" + nr_likes_str\n",
        "print (query_str)\n",
        "subdf=tweet_df.query(query_str)\n",
        "data_table.DataTable(subdf.head(20000), include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nuKpOVydwTd"
      },
      "source": [
        "#@title Choose Sample of Data for Visualization\n",
        "\n",
        "# limit of absolute_max=20000 due to colab DataTable\n",
        "\n",
        "import warnings\n",
        "absolute_max=20000\n",
        "\n",
        "display_labels=''\n",
        "\n",
        "if Data_Type == \"Humour Type\":\n",
        "  display_labels=names\n",
        "elif Data_Type == \"Humour Degree\" :\n",
        "  display_labels=degree_names\n",
        "elif Data_Type == \"Emotion2\" :\n",
        "  display_labels=emotion_names\n",
        "else :\n",
        "  display_labels=binary_degree_names\n",
        "\n",
        "as_many_as_possible = True #@param {type:\"boolean\"}\n",
        "sample = 1300 #@param {type:\"slider\", min:100, max:20000, step:100}\n",
        "op_max= sample\n",
        "\n",
        "if as_many_as_possible:\n",
        "  op_max = 20000\n",
        "\n",
        "likes_sorted=np.sort(tweet_df['likeCount'])\n",
        "num_tweets=len(tweet_df)\n",
        "divisor = min (absolute_max, op_max)\n",
        "\n",
        "last_like=0\n",
        "if absolute_max < num_tweets:\n",
        "  chunk_lst=np.array_split(likes_sorted,int(num_tweets/divisor))\n",
        "  last_like=int(chunk_lst[-1][0])\n",
        "\n",
        "tweet_df = tweet_df.dropna()\n",
        "subdf=tweet_df.loc[(tweet_df[Data_Type_name] !='not evaluated') & (tweet_df['likeCount'] > last_like)]\n",
        "\n",
        "# add line breaks to make tooltips readable\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    subdf.content = subdf.content.str.wrap(80)\n",
        "    subdf.content = subdf.content.apply(lambda x: x.replace('\\n', '<br>'))\n",
        "\n",
        "fig1 = px.scatter(subdf, x=\"retweetCount\", y=\"username\", size=\"likeCount\",\n",
        "                 color=Data_Type_name, hover_name=\"content\", facet_col=Data_Type_name, size_max=60, log_x=True, \n",
        "                 category_orders = {Data_Type_name: display_labels},\n",
        "                 color_discrete_sequence=color_palette, height=1500,opacity=0.5, facet_col_wrap=3)\n",
        "fig1.update_layout(\n",
        "    plot_bgcolor=plot_bgcolor,\n",
        "    title=\"Tweet Distribution in usage of \"+Data_Type+ \" amongst leading Twitterers\",\n",
        "    xaxis_title=\"Retweets (log-scale)\",\n",
        "    yaxis_title=\"Twitter Handles\",\n",
        "  )\n",
        "#fig1.for_each_annotation(lambda a: a.update(text=display_labels[int(a.text.split(\"=\")[-1][6:7])]))\n",
        "\n",
        "fig1.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyyBAXf12i2u"
      },
      "source": [
        "#@title Calculate Statistics \n",
        "#@markdown * For each type of Tweet propagation (reply, retweet, like, quote) \n",
        "#@markdown * And each trait type\n",
        "\n",
        "agg_fns = [np.mean,np.std,np.sum, np.median, np.count_nonzero, min, max]\n",
        "table = pd.pivot_table(tweet_df, values=['replyCount', 'retweetCount', 'likeCount', 'quoteCount'], index=[Data_Type_name],\n",
        "                    aggfunc={'replyCount': agg_fns,\n",
        "                             'retweetCount': agg_fns,\n",
        "                             'likeCount': agg_fns,\n",
        "                             'quoteCount': agg_fns,\n",
        "                             })\n",
        "table=table.loc[(table.index != \"not evaluated\")]\n",
        "# table.head(10)\n",
        "# print(table.columns)\n",
        "\n",
        "# need to rotate 'replyCount', 'retweetCount', 'likeCount', 'quoteCount' into one column and put the averages in a new column\n",
        "dic ={Data_Type_name:[], \"propagation type\":[],\"mean value\":[],\"Sum\":[],\"SD\":[],\"Median\":[],\"Count\":[],\"Min\":[],\"Max\":[]}\n",
        "for htype in table.iterrows():\n",
        "  # print (\"Htype:\",htype)\n",
        "  for i in range (0,4) : dic[Data_Type_name].append(htype[0])\n",
        "  dic[\"propagation type\"].append('likes')\n",
        "  dic[\"propagation type\"].append('quotes')\n",
        "  dic[\"propagation type\"].append('replies')\n",
        "  dic[\"propagation type\"].append('retweets')\n",
        "  for prop_type in ['likeCount','quoteCount','replyCount','retweetCount']:\n",
        "    dic[\"mean value\"].append(int(htype[1][prop_type,'mean']))\n",
        "    dic[\"SD\"].append(int(htype[1][prop_type,'std']))\n",
        "    dic[\"Median\"].append(int(htype[1][prop_type,'median']))\n",
        "    dic[\"Sum\"].append(int(htype[1][prop_type,'sum']))\n",
        "    dic[\"Count\"].append(int(htype[1][prop_type,'count_nonzero']))\n",
        "    dic[\"Min\"].append(int(htype[1][prop_type,'min']))\n",
        "    dic[\"Max\"].append(int(htype[1][prop_type,'max']))\n",
        "\n",
        "df_avg=pd.DataFrame(dic)\n",
        "\n",
        "df_avg.head(24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9u4vMHxFgqb",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize average numbers of Tweet propagations per humour type\n",
        "with_error_bars = False #@param {type:\"boolean\"}\n",
        "\n",
        "from plotly.graph_objs import *\n",
        "\n",
        "display_labels=''\n",
        "\n",
        "if Data_Type == \"Humour Type\":\n",
        "  display_labels=names\n",
        "  bar_title = \"Average Numbers of Tweet Propagations per Humour-Type\"\n",
        "elif Data_Type == \"Humour Degree\" :\n",
        "  display_labels=degree_names\n",
        "  bar_title = \"Average Numbers of Tweet Propagations per Humour-Degree\"\n",
        "elif Data_Type == \"Emotion2\" :\n",
        "  display_labels=emotion_names\n",
        "  bar_title = \"Average Numbers of Tweet Propagations per Emotion-Type\"\n",
        "else :\n",
        "  display_labels=binary_degree_names\n",
        "  bar_title = \"Average Numbers of Tweet Propagations when Non-Humorous/Humorous\"\n",
        "\n",
        "ebars = 'SD' if with_error_bars else None\n",
        "\n",
        "labels={'Humour': \"\",'propagation type': ''}\n",
        "fig = px.bar(df_avg, \n",
        "                   x='propagation type', \n",
        "                   y='mean value', \n",
        "                   facet_col=\"humour\",\n",
        "                  #  histfunc ='avg',\n",
        "                   color_discrete_sequence=color_palette_r,\n",
        "                   color='propagation type',\n",
        "                  #  barmode='group',\n",
        "                   category_orders = {'Humour': display_labels,\n",
        "                                      'propagation type': ['likes','retweets','replies','quotes']},\n",
        "                   labels=labels, \n",
        "                   error_y = ebars\n",
        "                   )\n",
        "fig.update_xaxes(type='category')\n",
        "\n",
        "fig.update_layout(\n",
        "    plot_bgcolor=plot_bgcolor,\n",
        "    title=bar_title,\n",
        "    xaxis_title=\"\",\n",
        "    yaxis_title=\"Mean Number of Tweet Propagations\",\n",
        "    # legend_title=\"Legend Title\"\n",
        "  )\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWydwjixEcqU"
      },
      "source": [
        "#@markdown ##Visualize Distribution of Tweets per Twitter-Handle\n",
        "#@markdown Select/deselect humour types in the legend to analyse deeper\n",
        "\n",
        "subdf2=tweet_df.query(Data_Type_name+\"!='not evaluated'\")\n",
        "s = subdf2.groupby(\"username\")[\"followers\"].sum().rank(ascending=True)\n",
        "# s = subdf2.groupby(\"username\").size().reset_index().groupby(['replyCount', 'retweetCount', 'likeCount', 'quoteCount']).sum().rank(ascending=True)\n",
        "\n",
        "fig = px.histogram(subdf2, \n",
        "                   x='username',\n",
        "                   color=Data_Type_name,\n",
        "                   color_discrete_sequence=color_palette_r,\n",
        "                   category_orders = {Data_Type_name: display_labels,\n",
        "                                      'propagation type': ['likes','retweets','replies','quotes'],\n",
        "                                      \"username\":s[s < 100000].sort_values().index.to_list()},\n",
        "                   orientation='v', height=800,\n",
        "                   labels=display_labels\n",
        "                      )\n",
        "fig.update_layout(\n",
        "    plot_bgcolor=plot_bgcolor,\n",
        "    title=\"Numbers of Tweets amongst leading Twitterers\",\n",
        "    )\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqcZVuJZE3n"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM1m1eRHZFX5"
      },
      "source": [
        "# Utility Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOXYY86WVbWt"
      },
      "source": [
        "# Invoke to show what gpu is in use\n",
        "gpu = !nvidia-smi\n",
        "gpu = '\\n'.join(gpu)\n",
        "print(gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEBmDqVX97zk"
      },
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsgNXvK3NF1d"
      },
      "source": [
        "%env TRAINING_FILE\n",
        "from datasets import load_dataset, load_metric\n",
        "datasets = load_dataset( \"csv\", delimiter='\\t', data_files=[train_file.value, test_file.value,  validation_file.value])\n",
        "\n",
        "len(datasets)\n",
        "datasets[\"train\"].column_names \n",
        "datasets[\"train\"]\n",
        "datasets[0]\n",
        "show_random_elements(datasets,1)\n",
        "train_file.value\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(train_file.value, delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hjtvx_Ydkj7"
      },
      "source": [
        "%env\n",
        "!python '/content/gdrive/MyDrive/ColabNotebooks/Visualization/run_glue2.py' --help\n",
        "train_file.value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFGc9Qs3A7AY"
      },
      "source": [
        "## Visualization of results in dashboard\n",
        "Analyze results (as they happen) on the project dashboard https://wandb.ai/jb-diplom/janice-demo\n",
        "\n",
        "### To retrieve models and their metadata from wandb\n",
        "\n",
        "1.   Go to the artifacts area of wandb (e.g. `https://wandb.ai/jb-diplom/janice-full/artifacts`)\n",
        "2.   Select the API Tag, which gives the precise code (below) for downloading the model that you need.\n",
        "3.   Check in the artifacts folder of Colab for the sub-folder (e.g. `model-12ai5jvy:0`) with the model.<br> Right-click it and take a copy of the path\n",
        "4.   Use the path in the huggingface pipeline constructor e.g.<br>\n",
        "`model_path=\"/content/artifacts/model-15ai5jvy:v0\"`\n",
        "`humour_classiffier = pipeline('sentiment-analysis',model_path)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKJtLLy3JM4"
      },
      "source": [
        "!pip install jupyter-dash\n",
        "\n",
        "import plotly.express as px\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output\n",
        "# Load Data\n",
        "df = px.data.tips()\n",
        "# Build App\n",
        "app = JupyterDash(__name__)\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"JupyterDash Demo\"),\n",
        "    dcc.Graph(id='graph'),\n",
        "    html.Label([\n",
        "        \"colorscale\",\n",
        "        dcc.Dropdown(\n",
        "            id='colorscale-dropdown', clearable=False,\n",
        "            value='plasma', options=[\n",
        "                {'label': c, 'value': c}\n",
        "                for c in px.colors.named_colorscales()\n",
        "            ])\n",
        "    ]),\n",
        "])\n",
        "# Define callback to update graph\n",
        "@app.callback(\n",
        "    Output('graph', 'figure'),\n",
        "    [Input(\"colorscale-dropdown\", \"value\")]\n",
        ")\n",
        "def update_figure(colorscale):\n",
        "    return px.scatter(\n",
        "        df, x=\"total_bill\", y=\"tip\", color=\"size\",\n",
        "        color_continuous_scale=colorscale,\n",
        "        render_mode=\"webgl\", title=\"Tips\"\n",
        "    )\n",
        "# Run app and display result inline in the notebook\n",
        "app.run_server(mode='inline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3an6Jd8Oavc"
      },
      "source": [
        "[humour-type dashboard](https://wandb.ai/jb-diplom/janice-final/reports/Dashboard-humour-type---Vmlldzo3MjE0NTI?accessToken=7pnh3o16evevab0a8abpt2th6ph0da4xg6575x2hoh3otkp8w1ch4sfbk2l54i0l)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfspv6pHO1zS"
      },
      "source": [
        "# Display W & B Destop for \n",
        "# own_modelid='electra-L-htype_balanced20e2021-05-16_07.13.44'\n",
        "run_id=\"jb-diplom/\"+ WandB_Project + \"/\" + own_modelid\n",
        "print(run_id)\n",
        "# run=wandb.init(project=WandB_Project, entity='jb-diplom', id=wandb.Api().run(run_id).id, resume='allow')\n",
        "run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some Test Code \n",
        "#@markdown This uses the ostensible batching API. But it doesn't seem (yet) to have any speed advantage\n",
        "##\n",
        "\n",
        "from datasets import Dataset\n",
        "from time import time\n",
        "\n",
        "sample_nr=1000  # in practice this and the other variables are already set above\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(content.head(sample_nr)))\n",
        "class_names=list(label_map.keys())\n",
        "\n",
        "start = time()\n",
        "batch_size = 512 # larger batch size bc distilled model is more memory efficient\n",
        "preds = []\n",
        "for i in tqdm(range(0, sample_nr, batch_size)):\n",
        "    examples = dataset[i:i+batch_size]['Text']\n",
        "    outputs = humour_classif(examples)\n",
        "    preds += [class_names.index(o['label']) for o in outputs]\n",
        "accuracy = np.mean(np.array(preds) == np.array(dataset['Text']))\n",
        "print(f\"Distilled model accuracy: {accuracy*100:0.2f}%\")\n",
        "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
      ],
      "metadata": {
        "id": "I-1HRpamYymv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}